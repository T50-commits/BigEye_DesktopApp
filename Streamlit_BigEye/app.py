import streamlit as st
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import pandas as pd
import time
import json
import os
import shutil
import subprocess
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone, timedelta
import config  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå config.py ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
import platform
import sys
import tkinter as tk
from tkinter import filedialog
import logging
import gc
import nltk
import tempfile
import atexit
from nltk.stem import SnowballStemmer
import base64
import hashlib

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Proxy files tracking for cleanup
_PROXY_FILES = set()
_PROXY_TEMP_DIR = None

def _cleanup_proxy_files():
    """‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå proxy ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏¥‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°"""
    global _PROXY_FILES, _PROXY_TEMP_DIR
    for proxy_path in list(_PROXY_FILES):
        try:
            if os.path.exists(proxy_path):
                os.remove(proxy_path)
                logging.info(f"Cleanup: removed proxy {proxy_path}")
        except Exception as e:
            logging.warning(f"Cleanup failed for {proxy_path}: {e}")
    _PROXY_FILES.clear()
    
    # ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå temp ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if _PROXY_TEMP_DIR and os.path.exists(_PROXY_TEMP_DIR):
        try:
            shutil.rmtree(_PROXY_TEMP_DIR)
            logging.info(f"Cleanup: removed temp dir {_PROXY_TEMP_DIR}")
        except Exception as e:
            logging.warning(f"Cleanup temp dir failed: {e}")

# ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô cleanup function
atexit.register(_cleanup_proxy_files)

# API Key Storage (Encrypted)
API_KEY_FILE = os.path.join(BASE_DIR, "api_key.enc")

def load_database() -> str:
    """
    ‡πÇ‡∏´‡∏•‡∏î Keyword Dictionary ‡∏à‡∏≤‡∏Å Server (RAM only)
    ‡πÑ‡∏°‡πà‡∏°‡∏µ local fallback - ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ internet connection
    """
    server_config = st.session_state.get('server_config', {})
    if server_config and server_config.get('dictionary'):
        return server_config['dictionary']
    
    logging.warning("Dictionary not available - server_config is empty")
    return ""

# ==========================================
# API KEY FUNCTIONS (Protected in compiled module)
# ==========================================
from license.validator_api import load_api_key as _load_api_key
from license.validator_api import save_api_key as _save_api_key
from license.validator_api import clear_api_key as _clear_api_key

def load_api_key() -> str:
    """‡πÇ‡∏´‡∏•‡∏î API Key (‡∏ñ‡∏≠‡∏î‡∏£‡∏´‡∏±‡∏™‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)"""
    return _load_api_key(BASE_DIR)

def save_api_key(api_key: str) -> bool:
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å API Key (‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)"""
    return _save_api_key(BASE_DIR, api_key)

def clear_api_key() -> bool:
    """‡∏•‡∏ö API Key"""
    return _clear_api_key(BASE_DIR)


# ==========================================
# SERVER CONFIG HELPERS (Prompts from Google Apps Script)
# ==========================================
def get_server_prompt(prompt_key: str) -> str:
    """
    ‡∏î‡∏∂‡∏á prompt ‡∏à‡∏≤‡∏Å server_config (RAM only)
    prompt_key: 'prompt_istock', 'prompt_hybrid', 'prompt_single'
    Returns: prompt string ‡∏´‡∏£‡∏∑‡∏≠ empty string ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö
    """
    server_config = st.session_state.get('server_config', {})
    if server_config:
        return server_config.get(prompt_key, '')
    return ''

def get_server_dictionary() -> str:
    """‡∏î‡∏∂‡∏á keyword dictionary ‡∏à‡∏≤‡∏Å server_config (RAM only)"""
    server_config = st.session_state.get('server_config', {})
    if server_config:
        return server_config.get('dictionary', '')
    return ''

def is_server_config_loaded() -> bool:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏´‡∏•‡∏î server config ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á"""
    server_config = st.session_state.get('server_config', {})
    return bool(server_config and server_config.get('prompt_istock'))


# ‡πÄ‡∏ä‡πá‡∏Ñ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NLTK ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡∏Å‡∏±‡∏ô Error)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    try:
        nltk.download('punkt', quiet=True)
    except Exception as e:
        logging.warning(f"Cannot download NLTK data: {e}")
        st.warning("‚ö†Ô∏è Cannot download NLTK data - some features may not work")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Error ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå debug_log.txt
logging.basicConfig(
    filename='debug_log.txt',
    filemode='a',  # 'a' = append (‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤)
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    level=logging.INFO,
    force=True,
    encoding='utf-8' # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
)

logging.info("=== Program Started (Session New) ===")
# --- [END] LOGGING SETUP ---

# ==========================================
# LICENSE CHECK (‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ)
# ==========================================
from license.validator_api import check_license
if not check_license(st, logging):
    st.stop()

# ==========================================
# 1. CONFIGURATION & CSS
# ==========================================
st.set_page_config(
    page_title="BigEye",
    page_icon="üì∏",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown(f"""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Sarabun:wght@400;700;900&display=swap');

    html, body, [class*="css"] {{
        font-family: 'Sarabun', sans-serif;
    }}

    .main-header {{
        font-size: 3.5rem;
        font-weight: 900;
        background: -webkit-linear-gradient(45deg, {config.THEME_COLOR_1}, {config.THEME_COLOR_2});
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 20px;
    }}
    .dev-credit {{
        font-size: 1rem; color: {config.THEME_COLOR_2}; text-align: center;
        font-weight: 400; letter-spacing: 2px; text-transform: uppercase;
        margin-bottom: 30px; opacity: 0.8;
    }}

    /* Radio Button Styling - ‡∏¢‡∏∑‡∏î‡πÄ‡∏ï‡πá‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á Sidebar */
    section[data-testid="stSidebar"] div[data-testid="stRadio"],
    section[data-testid="stSidebar"] div[data-testid="stRadio"] > div,
    section[data-testid="stSidebar"] div[data-testid="stRadio"] > div > div,
    section[data-testid="stSidebar"] div[role="radiogroup"] {{
        width: 100% !important;
        max-width: 100% !important;
    }}
    div[role="radiogroup"] {{
        display: flex !important;
        flex-direction: column !important;
        gap: 10px !important;
        width: 100% !important;
        margin-bottom: 15px !important;
    }}
    div[role="radiogroup"] label > div:first-child {{
        display: none !important;
    }}
    div[role="radiogroup"] label {{
        justify-content: center !important;
        text-align: center !important;
        background-color: white !important;
        border: 2px solid #e0e0e0 !important;
        border-radius: 10px !important;
        padding: 12px 15px !important;
        width: 100% !important;
        max-width: 100% !important;
        box-sizing: border-box !important;
        cursor: pointer !important;
        transition: all 0.2s ease !important;
        display: flex !important;
        align-items: center !important;
        margin: 0 !important;
        box-shadow: 0 4px 6px rgba(0,0,0,0.05) !important;
    }}
    div[role="radiogroup"] label:hover {{
        border-color: {config.THEME_COLOR_1} !important;
        transform: translateY(-2px);
    }}
    div[role="radiogroup"] label:has(input:checked) {{
        background: linear-gradient(90deg, {config.THEME_COLOR_1} 0%, {config.THEME_COLOR_2} 100%) !important;
        border: 2px solid transparent !important;
        transform: scale(1.02);
        box-shadow: 0 4px 12px rgba(51, 51, 255, 0.3) !important;
    }}
    div[role="radiogroup"] label:has(input:checked) * {{
        color: white !important;
        font-weight: 700 !important;
    }}

    /* Save & Clear Buttons */
    button[kind="primary"], button[kind="secondary"] {{
        height: 50px !important;
        padding: 0 20px !important;
        width: 100% !important;
        border-radius: 10px !important;
        font-size: 1.1rem !important;
        font-weight: 800 !important;
        display: flex !important;
        align-items: center !important;
        justify-content: center !important;
        transition: all 0.2s ease !important;
        line-height: 1 !important;
    }}
    button[kind="primary"] {{
        background: linear-gradient(90deg, {config.THEME_COLOR_1} 0%, {config.THEME_COLOR_2} 100%) !important;
        color: white !important;
        border: none !important;
        box-shadow: 0 4px 6px rgba(51, 51, 255, 0.2);
    }}
    button[kind="primary"]:hover {{
        transform: scale(1.02);
        box-shadow: 0 6px 15px rgba(51, 51, 255, 0.4);
    }}
    button[kind="secondary"] {{
        background-color: white !important;
        color: {config.STOP_COLOR} !important;
        border: 2px solid {config.STOP_COLOR} !important;
        box-sizing: border-box !important; 
    }}
    button[kind="secondary"]:hover {{
        background-color: {config.STOP_COLOR} !important;
        color: white !important;
        border-color: {config.STOP_COLOR} !important;
        transform: scale(1.02);
        box-shadow: 0 4px 10px rgba(255, 75, 75, 0.3);
    }}

    .success-box {{
        padding: 1rem; border-radius: 10px; background-color: #f0f7ff;
        border: 1px solid {config.THEME_COLOR_2}; color: {config.THEME_COLOR_2};
        text-align: center; margin-top: 15px;
    }}
</style>
""", unsafe_allow_html=True)


# ==========================================
# 2. CORE FUNCTIONS
# ==========================================

def cleanup_orphaned_files(api_key):
    if not api_key: return
    # Prevent multiple cleanup runs in same session
    if 'cleanup_done' in st.session_state and st.session_state['cleanup_done']:
        return
    try:
        genai.configure(api_key=api_key)
        deleted_count = 0
        now = datetime.now(timezone.utc)
        for f in genai.list_files():
            try:
                if f.create_time and (now - f.create_time > timedelta(hours=1)):
                    f.delete()
                    deleted_count += 1
            except:
                pass
        if deleted_count > 0:
            st.toast(f"üßπ Auto-Cleanup: {deleted_count} files", icon="‚ú®")
    except:
        pass

    try:
        from google.generativeai import caching
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏´‡∏≤ Cache ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
        for c in caching.CachedContent.list():
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Cache ‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏£‡∏≤‡πÑ‡∏´‡∏° (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏õ‡∏•‡∏ö‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏≠‡∏∑‡πà‡∏ô)
            if hasattr(c, 'display_name') and c.display_name == "istock_db_cache":
                try:
                    c.delete()
                    print(f"üßπ Auto-Cleaned Orphaned Cache: {c.name}")
                except Exception as e:
                    print(f"Failed to delete cache {c.name}: {e}")
    except ImportError:
        print("Google Generative AI caching not available")
    except Exception as e:
        print(f"Cache Cleanup Error: {e}")


def organize_output_files(source_folder, results, platform_name, filename_suffix, keyword_style=None):
    """
    ‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à:
    1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå 'Completed_[platform]_[style]_[timestamp]'
    2. Copy ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà
    3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå error_report.txt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
    
    Returns: (completed_folder_path, error_count, success_count)
    """
    from datetime import datetime
    
    # ‡πÅ‡∏¢‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô success ‡πÅ‡∏•‡∏∞ error
    success_files = [r for r in results if 'error' not in r]
    error_files = [r for r in results if 'error' in r]
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà (‡∏£‡∏ß‡∏° keyword_style ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    clean_platform = platform_name.replace(' ', '_').replace('&', 'and').replace('(', '').replace(')', '')
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° keyword_style ‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Adobe & Shutterstock)
    if keyword_style:
        clean_style = keyword_style.replace(' ', '_').replace('&', 'and').replace('(', '').replace(')', '')
        completed_folder_name = f"Completed_{clean_platform}_{clean_style}_{timestamp}"
    else:
        completed_folder_name = f"Completed_{clean_platform}_{timestamp}"
    completed_folder_path = os.path.join(source_folder, completed_folder_name)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
    try:
        os.makedirs(completed_folder_path, exist_ok=True)
    except Exception as e:
        logging.error(f"Failed to create completed folder: {e}")
        return None, len(error_files), len(success_files)
    
    # Copy ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà
    copied_count = 0
    for result in success_files:
        filename = result.get('file_name', '')
        if filename:
            source_path = os.path.join(source_folder, filename)
            dest_path = os.path.join(completed_folder_path, filename)
            
            try:
                if os.path.exists(source_path):
                    shutil.copy2(source_path, dest_path)
                    copied_count += 1
            except Exception as e:
                logging.error(f"Failed to copy {filename}: {e}")
    
    # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢
    for csv_file in os.listdir(source_folder):
        if csv_file.startswith("Metadata") and csv_file.endswith(".csv") and filename_suffix in csv_file:
            try:
                src_csv = os.path.join(source_folder, csv_file)
                dst_csv = os.path.join(completed_folder_path, csv_file)
                shutil.move(src_csv, dst_csv)
            except Exception as e:
                logging.error(f"Failed to move CSV {csv_file}: {e}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Error Report (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà Error)
    if error_files:
        error_report_path = os.path.join(completed_folder_path, f"error_report_{timestamp}.txt")
        try:
            with open(error_report_path, 'w', encoding='utf-8') as f:
                f.write("=" * 60 + "\n")
                f.write("           üìã ERROR REPORT - ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"üìÖ Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"üéØ Platform: {platform_name}\n")
                f.write(f"üìä Total Files Processed: {len(results)}\n")
                f.write(f"‚úÖ Success: {len(success_files)}\n")
                f.write(f"‚ùå Errors: {len(error_files)}\n")
                f.write("\n" + "=" * 60 + "\n")
                f.write("                    üìù ERROR DETAILS\n")
                f.write("=" * 60 + "\n\n")
                
                # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° Error ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                error_by_type = {}
                for err in error_files:
                    err_type = err.get('error_type', 'UNKNOWN')
                    if err_type not in error_by_type:
                        error_by_type[err_type] = []
                    error_by_type[err_type].append(err)
                
                # ‡πÅ‡∏™‡∏î‡∏á Error ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                for err_type, errors in error_by_type.items():
                    f.write(f"\nüî¥ [{err_type}] - {len(errors)} ‡πÑ‡∏ü‡∏•‡πå\n")
                    f.write("-" * 50 + "\n")
                    
                    for i, err in enumerate(errors, 1):
                        f.write(f"   {i}. {err.get('file_name', 'Unknown')}\n")
                        f.write(f"      ‚îî‚îÄ {err.get('error', 'Unknown error')}\n")
                        
                        # ‡πÅ‡∏™‡∏î‡∏á raw error ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                        if err.get('error_raw'):
                            f.write(f"      ‚îî‚îÄ [RAW] {err.get('error_raw')[:200]}...\n")
                        f.write("\n")
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
                f.write("\n" + "=" * 60 + "\n")
                f.write("              üí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤\n")
                f.write("=" * 60 + "\n\n")
                
                recommendations = {
                    "API_QUOTA_EXCEEDED": "‚Ä¢ ‡∏£‡∏≠ 1-2 ‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà\n‚Ä¢ ‡πÉ‡∏ä‡πâ API Key ‡∏≠‡∏∑‡πà‡∏ô\n‚Ä¢ ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Parallel Threads",
                    "RATE_LIMIT": "‚Ä¢ ‡∏•‡∏î Parallel Threads ‡∏•‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 1-2\n‚Ä¢ ‡∏£‡∏≠ 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà",
                    "TIMEOUT": "‚Ä¢ ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á\n‚Ä¢ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Internet\n‚Ä¢ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Video ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Proxy",
                    "PERMISSION_DENIED": "‚Ä¢ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ API Key ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n‚Ä¢ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ API ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß",
                    "INVALID_API_KEY": "‚Ä¢ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key ‡πÉ‡∏´‡∏°‡πà\n‚Ä¢ ‡∏™‡∏£‡πâ‡∏≤‡∏á API Key ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å Google AI Studio",
                    "JSON_PARSE_ERROR": "‚Ä¢ ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Model ‡∏≠‡∏∑‡πà‡∏ô (‡πÄ‡∏ä‡πà‡∏ô gemini-2.0-flash)\n‚Ä¢ ‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å",
                    "CONTENT_BLOCKED": "‚Ä¢ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n‚Ä¢ ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏û/‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏≠‡∏∑‡πà‡∏ô",
                    "NETWORK_ERROR": "‚Ä¢ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Internet\n‚Ä¢ ‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
                }
                
                for err_type in error_by_type.keys():
                    if err_type in recommendations:
                        f.write(f"üîß {err_type}:\n{recommendations[err_type]}\n\n")
                
                f.write("\n" + "=" * 60 + "\n")
                f.write("üìñ TIP: ‡∏î‡∏π debug_log.txt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n")
                f.write("=" * 60 + "\n")
        except Exception as e:
            logging.error(f"Failed to create error report: {e}")
    else:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Success Report ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        success_report_path = os.path.join(completed_folder_path, f"success_report_{timestamp}.txt")
        try:
            with open(success_report_path, 'w', encoding='utf-8') as f:
                f.write("=" * 60 + "\n")
                f.write("      üéâ SUCCESS REPORT - ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö!\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"üìÖ Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"üéØ Platform: {platform_name}\n")
                f.write(f"üìä Total Files Processed: {len(results)}\n")
                f.write(f"‚úÖ All Success: {len(success_files)} ‡πÑ‡∏ü‡∏•‡πå\n")
                f.write(f"‚ùå Errors: 0 ‡πÑ‡∏ü‡∏•‡πå\n\n")
                f.write("=" * 60 + "\n")
                f.write("              ‚ú® ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n")
                f.write("=" * 60 + "\n\n")
                
                for i, result in enumerate(success_files, 1):
                    filename = result.get('file_name', 'Unknown')
                    f.write(f"   {i}. ‚úÖ {filename}\n")
                
                f.write("\n" + "=" * 60 + "\n")
                f.write("   üéä ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏î‡πâ‡∏ß‡∏¢! ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î\n")
                f.write("=" * 60 + "\n")
        except Exception as e:
            logging.error(f"Failed to create success report: {e}")
    
    return completed_folder_path, len(error_files), len(success_files)


def select_folder_mac():
    try:
        script = '''
        tell application "System Events"
            activate
            set f to choose folder with prompt "Select Folder (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå)"
            return POSIX path of f
        end tell
        '''
        proc = subprocess.run(['osascript', '-e', script], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        if proc.returncode == 0:
            return proc.stdout.strip()
        else:
            return None
    except Exception as e:
        st.error(f"Mac Finder Error: {e}")
        return None


def get_dynamic_timeout(file_path, base_timeout):
    """Calculate timeout based on file size to handle different file sizes efficiently"""
    try:
        size_mb = os.path.getsize(file_path) / (1024*1024)
        # Add 10% more timeout per 10MB, max 2x base timeout
        multiplier = min(1 + (size_mb / 100), 2.0)
        return int(base_timeout * multiplier)
    except:
        return base_timeout


def enforce_istock_timecode(tc_str):
    """
    ‡πÅ‡∏õ‡∏•‡∏á Timecode ‡πÉ‡∏î‡πÜ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô format HH:MM:SS:FF ‡πÄ‡∏™‡∏°‡∏≠
    Input: "00:05", "0:05", "00:00:05", "5s"
    Output: "00:00:05:00"
    """
    if not tc_str or not isinstance(tc_str, str):
        return "00:00:00:00"

    # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏Ç‡∏¢‡∏∞‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô . ‡πÄ‡∏õ‡πá‡∏ô :
    clean_tc = tc_str.strip().replace(".", ":").replace("s", "")
    parts = clean_tc.split(":")

    # ‡∏Å‡∏£‡∏ì‡∏µ: ‡∏°‡∏≤‡πÅ‡∏Ñ‡πà MM:SS (‡πÄ‡∏ä‡πà‡∏ô 00:05) -> ‡πÄ‡∏ï‡∏¥‡∏° HH=00, FF=00
    if len(parts) == 2:
        return f"00:{parts[0].zfill(2)}:{parts[1].zfill(2)}:00"

    # ‡∏Å‡∏£‡∏ì‡∏µ: ‡∏°‡∏≤ HH:MM:SS (‡πÄ‡∏ä‡πà‡∏ô 00:00:05) -> ‡πÄ‡∏ï‡∏¥‡∏° FF=00
    elif len(parts) == 3:
        return f"{parts[0].zfill(2)}:{parts[1].zfill(2)}:{parts[2].zfill(2)}:00"

    # ‡∏Å‡∏£‡∏ì‡∏µ: ‡∏°‡∏≤‡∏Ñ‡∏£‡∏ö HH:MM:SS:FF ‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏•‡∏Ç 0 ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤)
    elif len(parts) == 4:
        return f"{parts[0].zfill(2)}:{parts[1].zfill(2)}:{parts[2].zfill(2)}:{parts[3].zfill(2)}"

    return "00:00:00:00"  # ‡∏Å‡∏£‡∏ì‡∏µ Error ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Default


def finalize_keywords_v5_ai_driven(keywords_list, target_count):
    """
    ARCHITECTURE V6: Smart Deduplication & Phrase Preservation
    - ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà AI ‡∏™‡πà‡∏á‡∏°‡∏≤ (‡∏ß‡∏•‡∏µ -> ‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß -> ‡∏ô‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°)
    - ‡πÉ‡∏ä‡πâ Stemming ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥ (Woman/Women, Run/Running)
    - ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏Å‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
    """

    # 1. SETUP TOOLS & BLACKLIST
    stemmer = SnowballStemmer("english")

    blacklist = {
        "filter", "presets", "instagram", "tiktok", "4k", "hd", "8k", "1080p",
        "macbook", "iphone", "samsung", "sony", "canon", "nikon",
        "facebook", "twitter", "youtube", "generated",
        "image", "photo", "picture", "shot", "concept", "view", "background",
        "of", "the", "a", "an", "with", "in", "on", "at", "by"
    }

    # Map ‡∏Ñ‡∏≥‡∏Å‡∏£‡∏¥‡∏¢‡∏≤/‡∏û‡∏´‡∏π‡∏û‡∏à‡∏ô‡πå‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏Å‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
    irregular_map = {
        "women": "woman", "men": "man", "children": "child",
        "people": "person", "feet": "foot", "teeth": "tooth",
        "mice": "mouse", "geese": "goose",
        "better": "good", "best": "good",
        "running": "run", "runner": "run", "runs": "run",
        "walking": "walk", "walked": "walk", "walks": "walk",
        "smiling": "smile", "smiled": "smile", "smiles": "smile",
        "working": "work", "worked": "work", "works": "work"
    }

    # --- PHASE 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á Map ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ stem ---
    stem_best_word = {}  # {stem: best_word} - ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ stem
    seen_phrases = set()  # ‡πÄ‡∏Å‡πá‡∏ö‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÅ‡∏•‡πâ‡∏ß
    processed_keywords = []  # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß

    for kw in keywords_list:
        # Clean text
        kw_clean = kw.lower().strip().strip(".").replace("-", " ")
        
        if not kw_clean or len(kw_clean) < 2 or kw_clean in blacklist:
            continue

        parts = kw_clean.split()

        # ‡∏Å‡∏£‡∏ì‡∏µ 1: ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß (Single Word)
        if len(parts) == 1:
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ - ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
            if kw_clean in irregular_map:
                kw_clean = irregular_map[kw_clean]
            
            stem_val = stemmer.stem(kw_clean)
            
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ stem ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
            if stem_val in stem_best_word:
                # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡πÉ‡∏´‡∏°‡πà‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ó‡∏ô
                if len(kw_clean) < len(stem_best_word[stem_val]):
                    stem_best_word[stem_val] = kw_clean
            else:
                stem_best_word[stem_val] = kw_clean
        
        # ‡∏Å‡∏£‡∏ì‡∏µ 2: ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏•‡∏µ (Phrase)
        else:
            if kw_clean not in seen_phrases:
                seen_phrases.add(kw_clean)
                processed_keywords.append(("phrase", kw_clean))

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
    for stem, word in stem_best_word.items():
        processed_keywords.append(("single", word))

    # --- PHASE 2: ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö (‡∏ß‡∏•‡∏µ‡∏Å‡πà‡∏≠‡∏ô) ---
    final_result = []
    added_stems = set()  # ‡πÄ‡∏Å‡πá‡∏ö stem ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏•‡πâ‡∏ß
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ß‡∏•‡∏µ‡∏Å‡πà‡∏≠‡∏ô
    for kw_type, kw in processed_keywords:
        if kw_type == "phrase":
            final_result.append(kw.title())
            if len(final_result) >= target_count:
                break
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß (‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥ stem)
    if len(final_result) < target_count:
        for kw_type, kw in processed_keywords:
            if kw_type == "single":
                stem = stemmer.stem(kw)
                if stem not in added_stems:
                    final_result.append(kw.title())
                    added_stems.add(stem)
                    if len(final_result) >= target_count:
                        break

    return final_result


def explode_phrases(keywords_list, target_count=45):
    """
    Adobe Strategy (Enhanced with Stemming Deduplication):
    1. Keep original phrases first (High Priority)
    2. Explode phrases into single words (appended at the end)
    3. Use stemming to detect duplicates - keep the best (shortest) word
    
    Input: ["woman running", "morning jog", "runner"]
    Output: ["Woman Running", "Morning Jog", "Woman", "Running", "Morning", "Jog"]
    (Note: "runner" is removed because "running" shares the same stem and is shorter/better)
    """
    if not keywords_list: return []
    
    stemmer = SnowballStemmer("english")
    
    # Map ‡∏Ñ‡∏≥‡∏Å‡∏£‡∏¥‡∏¢‡∏≤/‡∏û‡∏´‡∏π‡∏û‡∏à‡∏ô‡πå‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
    irregular_map = {
        "women": "woman", "men": "man", "children": "child",
        "people": "person", "feet": "foot", "teeth": "tooth",
        "mice": "mouse", "geese": "goose"
    }
    
    # Blacklist ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    blacklist = {"the", "and", "for", "a", "an", "of", "in", "on", "at", "to", "with", "by"}
    
    phrases = []  # ‡∏ß‡∏•‡∏µ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    single_words = []  # ‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
    stem_map = {}  # {stem: best_word} - ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ stem
    seen_phrases = set()  # ‡πÄ‡∏Å‡πá‡∏ö‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÅ‡∏•‡πâ‡∏ß (case-insensitive)
    
    # --- PHASE 1: ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏ß‡∏•‡∏µ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö ---
    for kw in keywords_list:
        clean = kw.strip().title()
        clean_lower = clean.lower()
        
        if clean and clean_lower not in seen_phrases:
            phrases.append(clean)
            seen_phrases.add(clean_lower)
    
    # --- PHASE 2: ‡πÅ‡∏ï‡∏Å‡∏ß‡∏•‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ä‡πá‡∏Ñ Stem ---
    for kw in keywords_list:
        clean_for_split = kw.replace("-", " ").strip()
        parts = clean_for_split.split()
        
        if len(parts) > 1:  # ‡πÅ‡∏ï‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏•‡∏µ (‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 1 ‡∏Ñ‡∏≥)
            for p in parts:
                p_clean = p.strip().title().strip(".,")
                p_lower = p_clean.lower()
                
                # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠ blacklist
                if len(p_clean) < 2 or p_lower in blacklist:
                    continue
                
                # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
                if p_lower in irregular_map:
                    p_clean = irregular_map[p_lower].title()
                    p_lower = p_clean.lower()
                
                # ‡∏´‡∏≤ stem ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏µ‡πâ
                stem = stemmer.stem(p_lower)
                
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ stem ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                if stem in stem_map:
                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß - ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤ (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤)
                    existing_word = stem_map[stem]
                    if len(p_clean) < len(existing_word):
                        stem_map[stem] = p_clean
                else:
                    stem_map[stem] = p_clean
    
    # --- PHASE 3: ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏ß‡∏•‡∏µ + ‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß) ---
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏ß‡∏•‡∏µ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô
    result = phrases.copy()
    seen_in_result = set(p.lower() for p in phrases)
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö‡∏ß‡∏•‡∏µ
    for stem, word in stem_map.items():
        if word.lower() not in seen_in_result:
            result.append(word)
            seen_in_result.add(word.lower())
    
    return result


def filter_stems(keywords_list):
    """
    Shutterstock Strategy:
    Input: ["run", "running", "runner"]
    Output: ["run"] (Keep strongest/simplest, prevent spam)
    """
    if not keywords_list: return []
    
    stemmer = SnowballStemmer("english")
    stem_map = {} # {stem: shortest_word}
    
    for kw in keywords_list:
        clean = kw.strip().title()
        if not clean: continue
            
        stem = stemmer.stem(clean.lower())
        
        # Logic: If stem not seen, add it.
        # If seen, keep the shorter word? (e.g. run < running)
        if stem not in stem_map:
            stem_map[stem] = clean
        else:
            current_best = stem_map[stem]
            if len(clean) < len(current_best): # Prefer shorter word
                stem_map[stem] = clean
                
    return list(stem_map.values())


def create_proxies_for_videos(file_paths):
    proxy_map = {}
    video_files = [p for p in file_paths if os.path.basename(p).lower().endswith(config.VALID_VIDEO_EXT)]
    if not video_files:
        return proxy_map

    progress = st.progress(0, text="Creating proxies...")
    total = len(video_files)
    for i, fp in enumerate(video_files, 1):
        if st.session_state.get('stop_flag'):
            break
        filename = os.path.basename(fp)
        progress.progress((i - 1) / max(total, 1), text=f"Creating proxy ({i}/{total}): {filename}")
        proxy_path = create_proxy_video(fp)
        if proxy_path and os.path.exists(proxy_path):
            proxy_map[fp] = proxy_path
            logging.info(f"Proxy Created Successfully: {filename}")
        else:
            logging.warning(f"Proxy Creation Failed for {filename}, using original file.")
        progress.progress(i / max(total, 1), text=f"Creating proxy ({i}/{total}): {filename}")

    if st.session_state.get('stop_flag'):
        progress.progress(1.0, text="Proxy creation stopped")
    else:
        progress.progress(1.0, text="Proxy creation completed")

    return proxy_map


def process_single_file(model, file_path, platform_config, db_content, keyword_count_val, title_limit, desc_limit, upload_path_override=None, keyword_mode=None, server_config=None):
    """
    Process a single file with AI analysis.
    server_config: dict containing prompts - passed directly because st.session_state is not accessible in worker threads
    """
    filename = os.path.basename(file_path)
    is_video = filename.lower().endswith(config.VALID_VIDEO_EXT)
    media_type_str = "VIDEO FOOTAGE" if is_video else "STILL PHOTO"
    video_instr = config.VIDEO_INSTRUCTION_TEXT if is_video else ""

    if platform_config["requires_db"]:
        keyword_input = db_content
    else:
        keyword_input = platform_config.get("keyword_placeholder_text", "")

    # --- SELECT PROMPT BASED ON MODE (From Server Config passed as parameter) ---
    # Note: st.session_state is NOT accessible in worker threads, so we use server_config parameter
    if not server_config:
        logging.error("CRITICAL: server_config is None! Cannot process without prompts.")
        return {"file_name": filename, "error": "Server config not available"}
    
    if platform_config.get('has_keyword_mode') and keyword_mode:
        # Adobe & Shutterstock mode - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å prompt ‡∏ï‡∏≤‡∏° keyword_mode
        if keyword_mode == "Single Words":
            prompt_template = server_config.get('prompt_single', '')
            prompt_key_used = 'prompt_single'
        else:  # HYBRID (Phrase & Single Words)
            prompt_template = server_config.get('prompt_hybrid', '')
            prompt_key_used = 'prompt_hybrid'
        
        # Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ prompt ‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not prompt_template:
            logging.error(f"CRITICAL: {prompt_key_used} is empty in server_config!")
            logging.error(f"server_config keys: {list(server_config.keys())}")
            return {"file_name": filename, "error": f"Prompt {prompt_key_used} not available"}
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì min_limit (75% ‡∏Ç‡∏≠‡∏á max_limit)
        title_min = int(title_limit * 0.75)
        desc_min = int(desc_limit * 0.75)
        
        # ‡∏Ç‡∏≠ keywords ‡πÄ‡∏û‡∏¥‡πà‡∏° 10 ‡∏Ñ‡∏≥ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ trim ‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á)
        keyword_request_count = keyword_count_val + 10
        
        prompt = prompt_template.format(
            media_type_str=media_type_str,
            video_instruction=video_instr,
            keyword_count=keyword_request_count,
            title_limit=title_limit,
            title_min=title_min,
            desc_limit=desc_limit,
            desc_min=desc_min
        )
    else:
        # iStock mode - ‡πÉ‡∏ä‡πâ prompt ‡∏à‡∏≤‡∏Å server_config parameter
        prompt_template = server_config.get('prompt_istock', '')
        
        # Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ prompt ‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not prompt_template:
            logging.error("CRITICAL: prompt_istock is empty in server_config!")
            logging.error(f"server_config keys: {list(server_config.keys())}")
            return {"file_name": filename, "error": "Prompt istock not available"}
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì title_min (75% ‡∏Ç‡∏≠‡∏á title_limit) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö iStock prompt
        title_min = int(title_limit * 0.75)
        
        prompt = prompt_template.format(
            media_type_str=media_type_str,
            video_instruction=video_instr,
            keyword_data=keyword_input,
            keyword_count=keyword_count_val + 15,  # ‡∏Ç‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° 15 ‡∏Ñ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏ö
            title_limit=title_limit,
            title_min=title_min,
            desc_limit=desc_limit
        )

    request_timeout = get_dynamic_timeout(file_path, config.TIMEOUT_VIDEO if is_video else config.TIMEOUT_PHOTO)
    uploaded_file = None

    try:
        upload_path = upload_path_override or file_path

        for attempt in range(config.MAX_RETRIES):
            try:
                uploaded_file = genai.upload_file(upload_path)

                start_wait = time.time()
                while uploaded_file.state.name == "PROCESSING":
                    time.sleep(5)
                    uploaded_file = genai.get_file(uploaded_file.name)
                    if time.time() - start_wait >= 300:
                        return {"file_name": filename, "error": "Timeout"}

                if uploaded_file.state.name == "FAILED":
                    return {"file_name": filename, "error": "Upload Failed"}

                response = model.generate_content(
                    [uploaded_file, prompt],
                    request_options={'timeout': request_timeout},
                    safety_settings={HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE}
                )

                text_resp = response.text
                json_str = ""
                match = re.search(r'```json\s*(\{.*?\})\s*```', text_resp, re.DOTALL)

                if match:
                    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ Backticks ‡∏Ñ‡∏£‡∏≠‡∏ö (‡πÄ‡∏≠‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÉ‡∏ô group 1)
                    json_str = match.group(1)
                else:
                    # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Backticks (‡∏´‡∏≤‡∏õ‡∏µ‡∏Å‡∏Å‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡∏õ‡∏¥‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏≠‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô group 0)
                    match = re.search(r'\{.*\}', text_resp, re.DOTALL)
                    if match:
                        json_str = match.group(0)
                    else:
                        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á Clean text ‡∏î‡∏¥‡∏ö‡πÜ ‡∏î‡∏π
                        json_str = text_resp.replace("```json", "").replace("```", "").strip()

                # ‡πÅ‡∏õ‡∏•‡∏á String ‡πÄ‡∏õ‡πá‡∏ô JSON
                data = json.loads(json_str)
                data['file_name'] = filename
                try:
                    # ‡πÅ‡∏õ‡∏•‡∏á string ‡πÄ‡∏õ‡πá‡∏ô list ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠
                    if "keywords" in data and isinstance(data["keywords"], str):
                        data["keywords"] = [k.strip() for k in data["keywords"].split(",")]

                    # --- ADOBE & SHUTTERSTOCK MODE (NEW) ---
                    if platform_config.get('has_keyword_mode') and keyword_mode:
                        # ‡πÇ‡∏´‡∏°‡∏î‡πÉ‡∏´‡∏°‡πà: AI ‡∏™‡πà‡∏á keywords ‡∏£‡∏ß‡∏°‡∏°‡∏≤‡πÉ‡∏ô data["keywords"]
                        raw_kw = data.get("keywords", [])
                        if isinstance(raw_kw, str):
                            raw_kw = [k.strip() for k in raw_kw.split(",")]
                        
                        # ‡πÉ‡∏ä‡πâ finalize_keywords_v5_ai_driven ‡πÄ‡∏û‡∏∑‡πà‡∏≠ stemming deduplication
                        processed_kw = finalize_keywords_v5_ai_driven(raw_kw, keyword_count_val)
                        data["keywords"] = processed_kw[:keyword_count_val]
                        
                        # ‡πÄ‡∏Å‡πá‡∏ö title ‡πÅ‡∏•‡∏∞ description ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
                        # (AI ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÉ‡∏ô data["title"] ‡πÅ‡∏•‡∏∞ data["description"] ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á)

                    elif not platform_config["requires_db"]:
                        # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ DB
                        data["keywords"] = finalize_keywords_v5_ai_driven(
                            data.get("keywords", []),
                            keyword_count_val
                        )

                    else:
                        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á iStock ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÑ‡∏ß‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ---
                        raw_kw = data.get("keywords", [])
                        cleaned_istock = []
                        for k in raw_kw:
                            k_clean = k.strip().strip(".")
                            if k_clean and len(k_clean) > 1:
                                cleaned_istock.append(k_clean)

                        data["keywords"] = cleaned_istock[:keyword_count_val]

                except Exception as e:
                    logging.error(f"Error filtering keywords for {filename}: {e}")
                # -------------------------------------------------------
                try:
                    if hasattr(response, 'usage_metadata') and response.usage_metadata:
                        data['token_input'] = response.usage_metadata.prompt_token_count
                        data['token_output'] = response.usage_metadata.candidates_token_count
                        data['token_total'] = response.usage_metadata.total_token_count
                    else:
                        data['token_input'] = 0
                        data['token_output'] = 0
                        data['token_total'] = 0
                except Exception:
                    data['token_input'] = 0
                    data['token_output'] = 0
                    data['token_total'] = 0
                return data


            except Exception as e:
                # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á Retry ‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
                if attempt < config.MAX_RETRIES - 1:
                    logging.warning(f"Retry {attempt + 1}/{config.MAX_RETRIES} for {filename} due to: {e}")
                    time.sleep(config.WAIT_TIME_RETRY * (attempt + 1))
                    continue
                
                # --- ENHANCED ERROR CLASSIFICATION ---
                error_str = str(e).lower()
                error_type = "UNKNOWN_ERROR"
                error_detail = str(e)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Error ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
                if "quota" in error_str or "resource exhausted" in error_str:
                    error_type = "API_QUOTA_EXCEEDED"
                    error_detail = "API Quota ‡πÄ‡∏ï‡πá‡∏° - ‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô API Key"
                elif "rate limit" in error_str or "429" in error_str:
                    error_type = "RATE_LIMIT"
                    error_detail = "‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏Ç‡∏≠‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ - ‡∏•‡∏≠‡∏á‡∏•‡∏î Parallel Threads"
                elif "timeout" in error_str or "deadline" in error_str:
                    error_type = "TIMEOUT"
                    error_detail = "‡∏´‡∏°‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠ - ‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≤‡∏à‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠ Network ‡∏ä‡πâ‡∏≤"
                elif "permission" in error_str or "403" in error_str:
                    error_type = "PERMISSION_DENIED"
                    error_detail = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key"
                elif "invalid" in error_str and "api" in error_str:
                    error_type = "INVALID_API_KEY"
                    error_detail = "API Key ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"
                elif "not found" in error_str or "404" in error_str:
                    error_type = "NOT_FOUND"
                    error_detail = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏£‡∏∑‡∏≠ Model"
                elif "json" in error_str or "parse" in error_str:
                    error_type = "JSON_PARSE_ERROR"
                    error_detail = "AI ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"
                elif "safety" in error_str or "blocked" in error_str:
                    error_type = "CONTENT_BLOCKED"
                    error_detail = "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÇ‡∏î‡∏¢ Safety Filter"
                elif "connection" in error_str or "network" in error_str:
                    error_type = "NETWORK_ERROR"
                    error_detail = "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Internet"
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Error ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå debug_log.txt
                logging.error(f"[{error_type}] {filename}: {error_detail}", exc_info=True)
                
                # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î Error
                return {
                    "file_name": filename, 
                    "error": f"[{error_type}] {error_detail}",
                    "error_type": error_type,
                    "error_raw": str(e)[:500]  # ‡πÄ‡∏Å‡πá‡∏ö raw error ‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏ß‡∏¢ (‡∏ï‡∏±‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 500 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
                }

    finally:
        if uploaded_file:
            try:
                uploaded_file.delete()
            except Exception as e:
                logging.warning(f"Failed to delete uploaded file: {e}")
        
        # Memory cleanup after processing each file
        gc.collect()

def play_notification_sound():
    st.markdown(
        """<audio autoplay><source src="https://assets.mixkit.co/active_storage/sfx/2869/2869-preview.mp3" type="audio/mpeg"></audio>""",
        unsafe_allow_html=True)


def create_proxy_video(input_path):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å (Proxy) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI
    - Resolution: 480p (Height)
    - Preset: Ultrafast (‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏£‡πá‡∏ß ‡πÑ‡∏°‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏ß‡∏¢)
    - CRF: 28 (‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏û‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å)
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô temp directory ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏£‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    """
    global _PROXY_FILES, _PROXY_TEMP_DIR
    try:
        filename = os.path.basename(input_path)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á temp directory ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        if _PROXY_TEMP_DIR is None or not os.path.exists(_PROXY_TEMP_DIR):
            _PROXY_TEMP_DIR = tempfile.mkdtemp(prefix="bigeye_proxy_")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô temp directory
        proxy_path = os.path.join(_PROXY_TEMP_DIR, f"proxy_{filename}")

        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà ‡∏•‡∏ö‡∏Å‡πà‡∏≠‡∏ô
        if os.path.exists(proxy_path):
            os.remove(proxy_path)

        # ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á FFmpeg
        # scale=-2:480 ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á‡πÄ‡∏õ‡πá‡∏ô 480p ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô (‡∏´‡∏≤‡∏£ 2 ‡∏•‡∏á‡∏ï‡∏±‡∏ß)
        base_path = os.path.dirname(os.path.abspath(__file__))

        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Windows, Mac ‡∏´‡∏£‡∏∑‡∏≠ Linux ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å FFmpeg ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
        ffmpeg_binary = None
        system = platform.system()
        machine = platform.machine()
        
        if system == "Windows":
            ffmpeg_path = os.path.join(base_path, "bin", "ffmpeg.exe")
            if os.path.exists(ffmpeg_path):
                ffmpeg_binary = ffmpeg_path
        elif system == "Darwin":  # macOS
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ FFmpeg ‡∏ï‡∏≤‡∏° architecture
            if machine == "arm64":  # Apple Silicon (M1-M4)
                ffmpeg_path = os.path.join(base_path, "bin", "macos-arm64", "ffmpeg")
            else:  # Intel Mac
                ffmpeg_path = os.path.join(base_path, "bin", "macos-x64", "ffmpeg")
            
            if os.path.exists(ffmpeg_path):
                ffmpeg_binary = ffmpeg_path
            else:
                # Fallback: ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô bin/macos-arm64 (Intel Mac ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡∏ú‡πà‡∏≤‡∏ô Rosetta 2)
                fallback_path = os.path.join(base_path, "bin", "macos-arm64", "ffmpeg")
                if os.path.exists(fallback_path):
                    ffmpeg_binary = fallback_path
        elif system == "Linux":
            ffmpeg_path = os.path.join(base_path, "bin", "ffmpeg")
            if os.path.exists(ffmpeg_path):
                ffmpeg_binary = ffmpeg_path
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô bin ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å system PATH
        if ffmpeg_binary is None:
            ffmpeg_binary = "ffmpeg"

        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á cmd ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ path ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏°‡∏≤‡πÑ‡∏î‡πâ
        cmd = [
            ffmpeg_binary,  # <--- ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô bin
            '-i', input_path,
            '-vf', 'scale=-2:480',
            '-c:v', 'libx264',
            '-preset', 'ultrafast',
            '-crf', '28',
            '-y',
            proxy_path
        ]

        # ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ö‡∏ö‡∏ã‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á (‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏î‡πâ‡∏á pop-up)
        result = subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        
        # Track ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cleanup ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏¥‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°
        _PROXY_FILES.add(proxy_path)
        return proxy_path

    except subprocess.CalledProcessError as e:
        logging.error(f"FFmpeg conversion failed for {filename}: {e}")
        return None
    except FileNotFoundError:
        logging.error("FFmpeg not found. Please install FFmpeg or ensure it's in the bin folder.")
        return None
    except Exception as e:
        logging.error(f"Proxy creation failed: {e}")
        return None  # ‡∏ñ‡πâ‡∏≤‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ None (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°)


def open_folder_selector():
    folder_path = None
    system_platform = platform.system()  # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏∞‡πÑ‡∏£

    # --- ‡∏Å‡∏£‡∏ì‡∏µ MAC (Darwin) ---
    if system_platform == "Darwin":
        try:
            # ‡πÉ‡∏ä‡πâ AppleScript ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Finder ‡πÅ‡∏ö‡∏ö Native
            script = """
            tell application "System Events"
                activate
                set f to choose folder with prompt "Select Folder (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏£‡∏π‡∏õ/‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠)"
                return POSIX path of f
            end tell
            """
            # ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á
            proc = subprocess.run(['osascript', '-e', script],
                                  stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            if proc.returncode == 0:
                folder_path = proc.stdout.strip()
            else:
                print(f"Mac Picker Cancelled/Error: {proc.stderr}")
        except subprocess.SubprocessError as e:
            logging.error(f"Mac AppleScript error: {e}")
        except Exception as e:
            logging.error(f"Mac System Error: {e}")

    # --- ‡∏Å‡∏£‡∏ì‡∏µ WINDOWS ---
    elif system_platform == "Windows":
        try:
            import tkinter as tk
            from tkinter import filedialog

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á Tkinter ‡πÅ‡∏ö‡∏ö‡∏ã‡πà‡∏≠‡∏ô (‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å)
            root = tk.Tk()
            root.withdraw()

            # [‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç] ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏î‡πâ‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà "‡∏ö‡∏ô‡∏™‡∏∏‡∏î" ‡πÄ‡∏™‡∏°‡∏≠ (Topmost)
            # ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Windows ‡∏ö‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏ä‡∏≠‡∏ö‡πÑ‡∏õ‡∏´‡∏•‡∏ö‡∏´‡∏•‡∏±‡∏á Chrome
            root.wm_attributes('-topmost', 1)

            # ‡πÄ‡∏õ‡∏¥‡∏î Dialog ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
            folder_path = filedialog.askdirectory(master=root)

            root.destroy()
        except ImportError:
            logging.error("Tkinter not available on this Windows system")
        except Exception as e:
            logging.error(f"Windows Picker Error: {e}")

    # --- ‡∏Å‡∏£‡∏ì‡∏µ LINUX ---
    elif system_platform == "Linux":
        try:
            # Try zenity first (most common on Linux desktops)
            result = subprocess.run(['zenity', '--file-selection', '--directory'], 
                               capture_output=True, text=True)
            if result.returncode == 0:
                folder_path = result.stdout.strip()
            else:
                # Fallback to console input
                folder_path = input("Enter folder path: ").strip()
        except FileNotFoundError:
            # Zenity not available, use console input
            try:
                folder_path = input("Enter folder path: ").strip()
            except:
                folder_path = None
        except Exception as e:
            logging.error(f"Linux Picker Error: {e}")

    return folder_path

# ==========================================
# 3. SIDEBAR (CONTROLS)
# ==========================================

if "my_api_key" not in st.session_state:
    # ‡πÇ‡∏´‡∏•‡∏î API Key ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™ (‡∏°‡∏µ auto-migration ‡∏à‡∏≤‡∏Å .txt ‡πÑ‡∏õ .enc)
    st.session_state.my_api_key = load_api_key()

with st.sidebar:
    st.title("üéõ BigEye Control Panel")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ License ‡πÅ‡∏•‡∏∞‡∏ß‡∏±‡∏ô‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏ (‡∏à‡∏≤‡∏Å session ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö online ‡πÅ‡∏•‡πâ‡∏ß)
    try:
        from datetime import datetime
        
        # Use days_left from server (more accurate)
        days_left = st.session_state.get('license_days_left', 0)
        expire_date = st.session_state.get('license_expire', '')
        
        if days_left > 0 or expire_date:
            if days_left <= 0 and expire_date:
                # Fallback to calculating from expire_date
                expiry = datetime.strptime(expire_date, '%d/%m/%Y')
                days_left = (expiry - datetime.now()).days
            
            if days_left <= 7:
                st.warning(f"‚ö†Ô∏è License ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ **{days_left}** ‡∏ß‡∏±‡∏ô")
            elif days_left <= 14:
                st.info(f"üìÖ License ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ **{days_left}** ‡∏ß‡∏±‡∏ô")
            else:
                st.success(f"‚úÖ License ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ **{days_left}** ‡∏ß‡∏±‡∏ô")
            
            if expire_date:
                st.caption(f"‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏: {expire_date}")
    except Exception:
        pass
    
    st.markdown("---")
    
    # [PERFORMANCE FIX] ‡∏£‡∏±‡∏ô cleanup ‡πÉ‡∏ô background thread ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ö‡∏•‡πá‡∏≠‡∏Å UI (30-40 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
    if st.session_state.my_api_key and "cleanup_done" not in st.session_state:
        import threading
        def run_cleanup_background():
            try:
                cleanup_orphaned_files(st.session_state.my_api_key)
            except:
                pass
        cleanup_thread = threading.Thread(target=run_cleanup_background, daemon=True)
        cleanup_thread.start()
        st.session_state.cleanup_done = True

    # 1. Google API Key
    api_input = st.text_input("üîë Google API Key", value=st.session_state.my_api_key, type="password")

    col_save, col_clear = st.columns(2)
    with col_save:
        if st.button("üíæ Save", type="primary", use_container_width=True):
            if save_api_key(api_input):
                st.session_state.my_api_key = api_input
                st.toast("API Key Saved! (‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡πÅ‡∏•‡πâ‡∏ß)", icon="‚úÖ")
                st.rerun()
            else:
                st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å API Key ‡πÑ‡∏î‡πâ")

    with col_clear:
        if st.button("üóë Clear", type="secondary", use_container_width=True):
            clear_api_key()
            st.session_state.my_api_key = ""
            st.toast("API Key Cleared!", icon="üóë")
            st.rerun()

    st.markdown("---")

    # 2. AI Model
    import google.generativeai as genai

    # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡∏à‡∏≥ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÇ‡∏î‡∏¢ gemini-2.5-pro ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (index=0)
    my_favorite_models = [
        "models/gemini-2.5-pro",        # Default - ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        "models/gemini-2.5-flash",       # ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î
        "models/gemini-2.0-flash",
        "models/gemini-3-pro-preview",
        "models/gemini-2.0-pro-exp-02-05"
    ]

    # [PERFORMANCE FIX] ‡πÉ‡∏ä‡πâ list ‡∏ï‡∏£‡∏á‡πÜ ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏ó‡∏µ‡πà‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å (30-40 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
    allowed_models = my_favorite_models

    model_choice = st.selectbox(
        "AI Model",
        options=allowed_models,
        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà gemini-2.5-pro (index=0)
        index=0
    )
    
    st.markdown("---")

    # 3. Select Mode
    st.subheader("üéØ Select Mode")
    
    # ‡πÉ‡∏ä‡πâ selectbox ‡πÅ‡∏ó‡∏ô radio ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏¢‡∏∑‡∏î‡πÄ‡∏ï‡πá‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á
    platform_options = list(config.PLATFORM_SETTINGS.keys())
    if 'platform_name' not in st.session_state:
        st.session_state.platform_name = platform_options[0]
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡πÅ‡∏ö‡∏ö full-width
    for i, option in enumerate(platform_options):
        is_selected = st.session_state.platform_name == option
        btn_type = "primary" if is_selected else "secondary"
        if st.button(option, key=f"mode_btn_{i}", use_container_width=True, type=btn_type):
            st.session_state.platform_name = option
            st.rerun()
    
    platform_name = st.session_state.platform_name

    st.session_state.platform_name = platform_name

    current_platform_cfg = config.PLATFORM_SETTINGS[platform_name]
    # ‡∏ã‡πà‡∏≠‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÇ‡∏´‡∏°‡∏î‡πÑ‡∏ß‡πâ (‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡πá‡∏ô)
    # mode_status = "üîí Strict Mode" if current_platform_cfg['requires_db'] else "‚ú® Open Mode"
    # st.caption(f"Status: {mode_status}")

    # --- KEYWORD MODE SELECTION (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Adobe & Shutterstock) ---
    if current_platform_cfg.get('has_keyword_mode'):
        st.markdown("---")
        st.subheader("üî§ Keyword Style")
        
        # Initialize keyword_mode in session state
        if 'keyword_mode' not in st.session_state:
            st.session_state.keyword_mode = "HYBRID (Phrase & Single Words)"
        
        keyword_mode_options = list(config.KEYWORD_MODE_OPTIONS.keys())
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Keyword Mode
        for i, kw_option in enumerate(keyword_mode_options):
            is_selected = st.session_state.keyword_mode == kw_option
            btn_type = "primary" if is_selected else "secondary"
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏´‡∏°‡∏î
            option_desc = config.KEYWORD_MODE_OPTIONS[kw_option].get('description', '')
            btn_label = f"{kw_option}"
            
            if st.button(btn_label, key=f"kw_mode_btn_{i}", use_container_width=True, type=btn_type):
                st.session_state.keyword_mode = kw_option
                st.rerun()

    # 4. Parallel Threads
    workers = st.slider("Parallel Threads", 1, 8, 4)

    st.markdown("---")
    
    # 5. Metadata Settings
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡πá‡∏≠‡∏Ñ settings
    is_processing = st.session_state.get('is_processing', False)
    
    if is_processing:
        st.subheader("üîí Metadata Settings (‡∏•‡πá‡∏≠‡∏Ñ)")
        st.caption("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
    else:
        st.subheader("üìè Metadata Settings")

    # --- 1. TITLE LIMIT ---
    if 'title_slider' not in st.session_state: 
        st.session_state.title_slider = 70
    if 'title_input' not in st.session_state:
        st.session_state.title_input = 70
    
    def update_title_slider():
        st.session_state.title_input = st.session_state.title_slider

    def update_title_input():
        st.session_state.title_slider = st.session_state.title_input

    c1, c2 = st.columns([3, 1])
    with c1:
        st.slider("Title Length", 50, 200, key="title_slider", on_change=update_title_slider, disabled=is_processing)
    with c2:
        st.number_input("Chars", 50, 200, key="title_input", on_change=update_title_input, label_visibility="collapsed", disabled=is_processing)
    
    title_char_limit = st.session_state.title_slider


    # --- 2. DESCRIPTION LIMIT ---
    if 'desc_slider' not in st.session_state: 
        st.session_state.desc_slider = 200
    if 'desc_input' not in st.session_state:
        st.session_state.desc_input = 200
    
    def update_desc_slider():
        st.session_state.desc_input = st.session_state.desc_slider

    def update_desc_input():
        st.session_state.desc_slider = st.session_state.desc_input

    c1, c2 = st.columns([3, 1])
    with c1:
        st.slider("Description Length", 100, 500, key="desc_slider", on_change=update_desc_slider, disabled=is_processing)
    with c2:
        st.number_input("Chars", 100, 500, key="desc_input", on_change=update_desc_input, label_visibility="collapsed", disabled=is_processing)
    
    desc_char_limit = st.session_state.desc_slider


    # --- 3. KEYWORD COUNT ---
    if 'kw_slider' not in st.session_state: st.session_state.kw_slider = 45
    if 'kw_input' not in st.session_state: st.session_state.kw_input = 45
    
    def update_kw_slider():
        st.session_state.kw_input = st.session_state.kw_slider

    def update_kw_input():
        st.session_state.kw_slider = st.session_state.kw_input

    c1, c2 = st.columns([3, 1])
    with c1:
        st.slider("Keywords Count", 10, 50, key="kw_slider", on_change=update_kw_slider, disabled=is_processing)
    with c2:
        st.number_input("Count", 10, 50, key="kw_input", on_change=update_kw_input, label_visibility="collapsed", disabled=is_processing)

    target_kw_count = st.session_state.kw_slider
    # Negative Keywords ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß
    
    st.markdown("---")
    
    # 6. Debug Tools
    st.subheader("üîß Debug Tools")
    if st.button("üïµÔ∏è Check Active Caches"):
        try:
            genai.configure(api_key=st.session_state.my_api_key)

            from google.generativeai import caching

            # ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ Google List ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Cache ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            caches = list(caching.CachedContent.list())

            if not caches:
                st.success("‚úÖ Clean! ‡πÑ‡∏°‡πà‡∏û‡∏ö Cache ‡∏Ñ‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö")
            else:
                st.warning(f"‚ö†Ô∏è ‡∏û‡∏ö Cache ‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà {len(caches)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£:")
                for c in caches:
                    st.caption(f"- {c.name} (Exp: {c.expire_time})")
                    # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏•‡∏ö‡∏°‡∏∑‡∏≠ ‡∏Å‡πá‡∏™‡∏±‡πà‡∏á‡∏•‡∏ö‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
                    # c.delete()
        except Exception as e:
            st.error(f"Error checking cache: {e}")

    # ======================================================
    # 7. Keyword Database (‡∏ã‡πà‡∏≠‡∏ô‡∏à‡∏≤‡∏Å UI ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà)
    # ======================================================
    db_content = ""
    current_platform_cfg = config.PLATFORM_SETTINGS[st.session_state.platform_name]
    if current_platform_cfg["requires_db"]:
        db_content = load_database()  # ‡πÇ‡∏´‡∏•‡∏î‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á (‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á UI)

# ==========================================
# 4. MAIN INTERFACE
# ==========================================

st.markdown(
    f'<h1 class="main-header">BigEye <span style="font-size:0.4em; vertical-align: middle; color:#333;">(Pro)</span></h1>',
    unsafe_allow_html=True)

# --- FOLDER PICKER ---

# --- FOLDER SELECTION UI (Universal Fix) ---


# ‡∏à‡∏±‡∏î‡∏ß‡∏≤‡∏á‡∏õ‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
col_btn, col_input = st.columns([1, 4])

with col_btn:
    # 1. ‡∏õ‡∏∏‡πà‡∏°‡∏Å‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (Pop-up)
    if st.button("Browse...", type="primary", use_container_width=True):
        selected = open_folder_selector()
        if selected:
            st.session_state['folder_path'] = selected
            st.rerun()

with col_input:
    # 2. ‡∏ä‡πà‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å Path (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Paste)
    current_val = st.session_state.get('folder_path', '')

    new_path_input = st.text_input(
        "Folder Path",
        value=current_val,
        label_visibility="collapsed",
        placeholder="Paste path here..."
    )

    # --- [FIX] ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Mac ‡πÅ‡∏ñ‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ ' ‡∏°‡∏≤‡πÉ‡∏´‡πâ ---
    if new_path_input:
        # 1. .strip() ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏±‡∏ß‡∏ó‡πâ‡∏≤‡∏¢
        # 2. .strip("'") ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ù‡∏ô‡∏ó‡∏≠‡∏á ' ‡∏ó‡∏µ‡πà Mac ‡πÅ‡∏ñ‡∏°‡∏°‡∏≤
        # 3. .strip('"') ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ü‡∏±‡∏ô‡∏´‡∏ô‡∏π " (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏°‡∏µ)
        clean_path = new_path_input.strip().strip("'").strip('"')
    else:
        clean_path = ""
    # ------------------------------------------------

    # Logic: ‡∏ñ‡πâ‡∏≤ User ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ User ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏™‡∏±‡πà‡∏á
    if clean_path != current_val:
        st.session_state['folder_path'] = clean_path
        st.rerun()

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Path ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏≤‡∏Å‡∏õ‡∏∏‡πà‡∏° ‡∏´‡∏£‡∏∑‡∏≠ ‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏≠‡∏á) ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°
final_path = st.session_state.get('folder_path', '')
if final_path:
    if os.path.isdir(final_path):
        st.success(f"‚úÖ Selected: {final_path}")
    else:
        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ (Path Incorrect)")
        final_path = None  # reset ‡∏ñ‡πâ‡∏≤ path ‡∏ú‡∏¥‡∏î

# ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠
current_path = final_path

# --- MAIN LOGIC ---
if current_path and os.path.isdir(current_path):
    # 1. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
    all_items = [f for f in os.listdir(current_path) if not f.startswith('.')]
    valid_exts = config.VALID_IMAGE_EXT + config.VALID_VIDEO_EXT

    # -------------------------------------------------------------
    # [FIXED LOGIC] ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå Proxy (‡∏Å‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞‡∏´‡∏•‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏≤ CSV)
    # -------------------------------------------------------------

    # Step A: ‡∏•‡πâ‡∏≤‡∏á‡∏ö‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Proxy ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å‡∏Ñ‡πâ‡∏≤‡∏á‡∏ó‡∏¥‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô (Auto-Cleanup)
    # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î Stop ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏î‡∏±‡∏ö‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏£‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
    for f in all_items:
        if f.startswith("proxy_"):
            try:
                os.remove(os.path.join(current_path, f))
                logging.info(f"Cleaned up old proxy: {f}")
            except PermissionError:
                logging.warning(f"Permission denied removing proxy: {f}")
            except FileNotFoundError:
                pass  # File already deleted
            except Exception as e:
                logging.error(f"Failed to remove proxy {f}: {e}")

    # Step B: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á (Ignore Proxy)
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç 'and not f.startswith("proxy_")' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå 100% ‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏¥‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞‡∏°‡∏≤‡∏ó‡∏≥
    target_files = [
        os.path.join(current_path, f)
        for f in all_items
        if f.lower().endswith(valid_exts) and not f.startswith("proxy_")
    ]

    st.info(f"üìÇ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: **{len(target_files)}** ‡πÑ‡∏ü‡∏•‡πå | ‡πÇ‡∏´‡∏°‡∏î: **{platform_name}**")

    if 'is_processing' not in st.session_state:
        st.session_state['is_processing'] = False
    if 'completion_summary' not in st.session_state:
        st.session_state['completion_summary'] = None

    controls_placeholder = st.empty()

    def _render_controls():
        with controls_placeholder.container():
            c_start, c_stop = st.columns([3, 1])
            with c_start:
                start_label = "‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô..." if st.session_state['is_processing'] else f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ({platform_name})"
                start_disabled = (len(target_files) == 0) or st.session_state['is_processing']
                start_clicked = st.button(start_label, disabled=start_disabled,
                                          use_container_width=True, type="primary")
            with c_stop:
                stop_clicked = st.button("üõë STOP", use_container_width=True, type="secondary", disabled=not st.session_state['is_processing'])
        return start_clicked, stop_clicked

    start_process, stop_process = _render_controls()

    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    if st.session_state.get('completion_summary'):
        summary = st.session_state['completion_summary']
        st.success(f"""
### [OK] ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô

| ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô |
|--------|-------|
| [IMG] ‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢ | {summary['photo_count']} ‡πÑ‡∏ü‡∏•‡πå |
| [VDO] ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ | {summary['video_count']} ‡πÑ‡∏ü‡∏•‡πå |
| **‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î** | **{summary['total_count']} ‡πÑ‡∏ü‡∏•‡πå** |

[FOLDER] **‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà:**  
`{summary['output_folder']}`
""")
        # ‡∏•‡πâ‡∏≤‡∏á summary ‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏•‡πâ‡∏ß (‡∏à‡∏∞‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÄ‡∏°‡∏∑‡πà‡∏≠ user ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡πÉ‡∏´‡∏°‡πà)
        st.session_state['completion_summary'] = None

    if stop_process:
        st.session_state['stop_flag'] = True
    else:
        if 'stop_flag' not in st.session_state:
            st.session_state['stop_flag'] = False

    if start_process:
        st.session_state['stop_flag'] = False
        st.session_state['is_processing'] = True
        st.rerun()

    if st.session_state.get('is_processing'):
        if not st.session_state.my_api_key:
            st.error("‚ùå ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏™‡πà API Key (‡∏î‡∏π‡πÄ‡∏°‡∏ô‡∏π‡∏ã‡πâ‡∏≤‡∏¢‡∏°‡∏∑‡∏≠)")
            st.session_state['is_processing'] = False
        elif current_platform_cfg["requires_db"] and not db_content:
            st.error("‚ùå ‡πÇ‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Database (‡∏î‡∏π‡πÄ‡∏°‡∏ô‡∏π‡∏ã‡πâ‡∏≤‡∏¢‡∏°‡∏∑‡∏≠)")
            st.session_state['is_processing'] = False
        else:
            results = []
            run_state = "error"
            try:
                genai.configure(api_key=st.session_state.my_api_key)
                model = genai.GenerativeModel(model_name=model_choice,
                                              generation_config={"response_mime_type": "application/json"})

                from google.generativeai import caching
                import datetime

                CACHE_THRESHOLD = 10

                if platform_name == "iStock" and db_content and len(target_files) >= CACHE_THRESHOLD:
                    try:
                        st.info(f"üöÄ Large Batch Detected ({len(target_files)} files) -> Initializing Context Cache...")

                        sys_instruction = f"""
                            You are an expert Stock Keyword Generator for BigEye.
                            CRITICAL INSTRUCTION: You have been provided with a massive 'Reference Dictionary' in this System Context.
                            When asked to generate keywords, you MUST STRICTLY use words from this cached dictionary only.

                            --- REFERENCE DICTIONARY START ---
                            {db_content}
                            --- REFERENCE DICTIONARY END ---
                            """

                        cache = caching.CachedContent.create(
                            model=model_choice,
                            display_name="istock_db_cache",
                            system_instruction=sys_instruction,
                            ttl=datetime.timedelta(minutes=60),
                        )

                        model = genai.GenerativeModel.from_cached_content(cached_content=cache)
                        db_content = "(*** REFERENCE DICTIONARY IS CACHED IN SYSTEM CONTEXT - DO NOT REPEAT ***)"

                        st.success("‚úÖ Context Caching Active! (Database loaded into memory)")
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è Caching Skipped (Standard Mode): {e}")

                proxy_map = create_proxies_for_videos(target_files)

                # Check if stopped during proxy creation
                if st.session_state.get('stop_flag'):
                    run_state = "stopped"
                    st.warning("üõë ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (Stopped during proxy creation)")
                else:
                    run_state = "completed"
                    with st.status("ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå... (AI Working)", expanded=True) as status:
                        progress_bar = st.progress(0, text="Uploading files...")
                        # ‡∏î‡∏∂‡∏á keyword_mode ‡∏à‡∏≤‡∏Å session state (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Adobe & Shutterstock)
                        current_keyword_mode = st.session_state.get('keyword_mode', None)
                        # ‡∏î‡∏∂‡∏á server_config ‡∏à‡∏≤‡∏Å session state ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÑ‡∏õ worker threads
                        # (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ st.session_state ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å worker threads)
                        current_server_config = st.session_state.get('server_config', {})
                        
                        if not current_server_config:
                            st.error("‚ùå Server config ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ Refresh ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö")
                            run_state = "error"
                        else:
                            with ThreadPoolExecutor(max_workers=workers) as executor:
                                future_to_file = {
                                    executor.submit(
                                        process_single_file,
                                        model,
                                        f,
                                        current_platform_cfg,
                                        db_content,
                                        target_kw_count,
                                        title_char_limit,
                                        desc_char_limit,
                                        proxy_map.get(f),
                                        current_keyword_mode,  # ‡∏™‡πà‡∏á keyword_mode ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
                                        current_server_config  # ‡∏™‡πà‡∏á server_config ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢ (thread-safe)
                                    ): f for f in target_files
                                }
                                for i, future in enumerate(as_completed(future_to_file)):
                                    if st.session_state.get('stop_flag'):
                                        run_state = "stopped"
                                        executor.shutdown(wait=False, cancel_futures=True)
                                        progress_bar.progress((i + 1) / len(target_files), text="‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
                                        status.update(label="üõë ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", state="error")
                                        break
                                    data = future.result()
                                    results.append(data)
                                    progress_bar.progress((i + 1) / len(target_files), text=f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ({i+1}/{len(target_files)}): {data.get('file_name','')}")

                            if run_state == "completed":
                                progress_bar.progress(1.0, text="‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
                                status.update(label="‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô", state="complete")

                for original_path, proxy_path in proxy_map.items():
                    try:
                        if proxy_path and os.path.exists(proxy_path):
                            os.remove(proxy_path)
                    except Exception as e:
                        logging.warning(f"Failed to delete proxy {proxy_path}: {e}")

                if run_state == "completed" and results:
                    play_notification_sound()
                    st.balloons()
                elif run_state == "stopped":
                    st.info("üõë Stopped")

                if run_state == "completed" and results:
                    success_data = [r for r in results if "error" not in r]

                    if success_data:
                        df = pd.DataFrame(success_data)

                        # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                        df.rename(columns={"file_name": "Filename"}, inplace=True)

                        # ‡πÅ‡∏õ‡∏•‡∏á List ‡πÉ‡∏ô Keywords ‡πÄ‡∏õ‡πá‡∏ô String (comma-separated)
                        if "keywords" in df.columns:
                            df["keywords"] = df["keywords"].apply(lambda x: ", ".join(x) if isinstance(x, list) else x)

                        # ---------------------------------------------------------
                        # LOGIC ‡∏Å‡∏≤‡∏£‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î (Fixed for iStock Template)
                        # ---------------------------------------------------------
                        from datetime import datetime

                        # ‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (Format: YYYY-MM-DD)
                        current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

                        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©
                        clean_model = model_choice.replace("models/", "").replace(":", "").replace(".", "-")

                        # ‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ó‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå
                        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: "_gemini-2-0-flash_2024-02-14_14-30-05"
                        filename_suffix = f"_{clean_model}_{current_datetime}"

                        if platform_name == "Adobe & Shutterstock":
                            # ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á Adobe/Shutterstock (‡πÅ‡∏¢‡∏Å 2 ‡πÑ‡∏ü‡∏•‡πå - ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
                            df_base = df.copy()

                            # 1. Adobe Version (Title + Keywords)
                            df_adobe = df_base.copy()
                            df_adobe.rename(columns={"title": "Title", "keywords": "Keywords"}, inplace=True)
                            
                            # ‡πÅ‡∏õ‡∏•‡∏á Keywords list ‡πÄ‡∏õ‡πá‡∏ô comma-separated string
                            if "Keywords" in df_adobe.columns:
                                df_adobe["Keywords"] = df_adobe["Keywords"].apply(
                                    lambda x: ", ".join(x) if isinstance(x, list) else x
                                )
                            
                            df_adobe["Category"] = ""
                            df_adobe["Releases"] = ""
                            final_adobe = df_adobe.reindex(columns=config.ADOBE_CSV_COLUMNS)
                            final_adobe.to_csv(os.path.join(current_path, f"Metadata Adobe{filename_suffix}.csv"), index=False, encoding='utf-8')

                            # 2. Shutterstock Version (Description + Keywords)
                            df_ss = df_base.copy()
                            df_ss.rename(columns={"description": "Description", "keywords": "Keywords"}, inplace=True)
                            
                            # ‡πÅ‡∏õ‡∏•‡∏á Keywords list ‡πÄ‡∏õ‡πá‡∏ô comma-separated string
                            if "Keywords" in df_ss.columns:
                                df_ss["Keywords"] = df_ss["Keywords"].apply(
                                    lambda x: ", ".join(x) if isinstance(x, list) else x
                                )
                            
                            df_ss["Categories"] = ""
                            df_ss["Illustration"] = "No"
                            df_ss["Mature Content"] = "No"
                            df_ss["Editorial"] = "No"
                            final_ss = df_ss.reindex(columns=config.SHUTTERSTOCK_CSV_COLUMNS)
                            final_ss.to_csv(os.path.join(current_path, f"Metadata Shutterstock{filename_suffix}.csv"), index=False, encoding='utf-8')

                        elif platform_name == "iStock":
                            df_istock_base = df.copy()
                            is_video_mask = df_istock_base['Filename'].str.lower().str.endswith(config.VALID_VIDEO_EXT)
                            df_photo = df_istock_base[~is_video_mask]
                            df_video = df_istock_base[is_video_mask]
                            if not df_photo.empty:
                                df_p = df_photo.copy()
                                df_p.rename(columns={
                                    "Filename": "file name",
                                    "niche_analysis": "Niche Strategy",
                                    "missing_keywords": "Missing Keywords"
                                }, inplace=True)
                                final_photo = df_p.reindex(columns=config.ISTOCK_COLS_PHOTO)
                                final_photo.to_csv(os.path.join(current_path, f"Metadata iStock Photos{filename_suffix}.csv"), index=False, encoding='utf-8')
                            if not df_video.empty:
                                df_v = df_video.copy()
                                df_v.rename(columns={
                                    "Filename": "file name",
                                    "missing_keywords": "Missing Keywords",
                                    "poster_timecode": "poster timecode",
                                    "shot_speed": "shot speed"
                                }, inplace=True)
                                final_video = df_v.reindex(columns=config.ISTOCK_COLS_VIDEO)
                                final_video.to_csv(os.path.join(current_path, f"Metadata iStock Videos{filename_suffix}.csv"), index=False, encoding='utf-8')

                        if 'cache' in locals() and cache:
                            try:
                                cache.delete()
                                st.toast("üßπ Cache Deleted (Cost Saving)", icon="üí∏")
                                print(f"Deleted cache: {cache.name}")
                            except Exception as e:
                                print(f"Could not delete cache: {e}")

                        with st.spinner("üìÅ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡πÑ‡∏ü‡∏•‡πå..."):
                            completed_folder, error_count, success_count = organize_output_files(
                                source_folder=current_path,
                                results=results,
                                platform_name=platform_name,
                                filename_suffix=filename_suffix,
                                keyword_style=current_keyword_mode  # ‡∏™‡πà‡∏á keyword style ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
                            )

                        # ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏ß‡πâ‡πÉ‡∏ô session state
                        video_count = sum(1 for r in success_data if r.get('file_name', '').lower().endswith(config.VALID_VIDEO_EXT))
                        photo_count = len(success_data) - video_count
                        st.session_state['completion_summary'] = {
                            'photo_count': photo_count,
                            'video_count': video_count,
                            'total_count': len(success_data),
                            'output_folder': completed_folder
                        }
                        
                        # Report usage to server (async - non-blocking)
                        if st.session_state.get('license_key'):
                            try:
                                from license.validator_api import report_usage
                                report_usage(
                                    st.session_state.license_key,
                                    photo_count=photo_count,
                                    video_count=video_count
                                )
                            except Exception as e:
                                logging.warning(f"Failed to report usage: {e}")

            except Exception as e:
                logging.error(f"Processing failed: {e}")
                st.error(f"‚ùå Processing failed: {e}")
            finally:
                st.session_state['is_processing'] = False
                st.session_state['stop_flag'] = False
                controls_placeholder.empty()
                st.rerun()

elif not current_path:
    st.info("üëà ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô")
else:
    st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {current_path}")